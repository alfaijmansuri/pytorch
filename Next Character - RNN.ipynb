{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''----------------------------------------------------------------------------------------'''\n",
    "\n",
    "with open('anna.txt','r') as f:\n",
    "    text = f.read()\n",
    "text[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 64, 71, 36, 63,  1, 29,  5, 44, 33, 33, 33, 42, 71, 36, 36, 59,\n",
       "        5, 10, 71, 35, 39, 74, 39,  1, 53,  5, 71, 29,  1,  5, 71, 74, 74,\n",
       "        5, 71, 74, 39, 65,  1, 38,  5,  1, 27,  1, 29, 59,  5, 28, 25, 64,\n",
       "       71, 36, 36, 59,  5, 10, 71, 35, 39, 74, 59,  5, 39, 53,  5, 28, 25,\n",
       "       64, 71, 36, 36, 59,  5, 39, 25,  5, 39, 63, 53,  5,  4, 66, 25, 33,\n",
       "       66, 71, 59, 46, 33, 33,  0, 27,  1, 29, 59, 63, 64, 39, 25])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "if False:\n",
    "    def one_hot_encode(arr, n_labels):\n",
    "\n",
    "        # Initialize the the encoded array\n",
    "        #print(arr.shape)\n",
    "        print(np.multiply(*arr.shape))\n",
    "        one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "        print(\"array one hot shape\")\n",
    "        print(one_hot.shape[0]) \n",
    "        print(\"-------------------------------------------------------\")\n",
    "        print(arr.flatten())\n",
    "        print(\"--------------------------------------------------------\")\n",
    "        print(\"one hot\",one_hot)\n",
    "        print(\"[[0,1,2],[3,5,1]]\")\n",
    "\n",
    "        print( one_hot[np.arange(one_hot.shape[0]), arr.flatten()])\n",
    "\n",
    "        # Fill the appropriate elements with ones\n",
    "        one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "        print(one_hot)\n",
    "\n",
    "        # Finally reshape it to get back to the original array\n",
    "        one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded,50,8)\n",
    "x,y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[32 64 71 36 63  1 29  5]\n",
      " [29  5 51  4 46 33 33 45]\n",
      " [41 71 74 74  1 22  5 71]\n",
      " [64  1  5 53 63 29 71 25]\n",
      " [ 5 27  4 22 65 71 12  5]\n",
      " [74  5  4 10  5 71  5 36]\n",
      " [ 1  5 53 71 39 22 12  5]\n",
      " [ 4 25  5 71 25 22  5 40]\n",
      " [51  5  4 10  5 63 64  1]\n",
      " [71 63  5 53 63 71 39 29]]\n",
      "\n",
      "y\n",
      " [[64 71 36 63  1 29  5 44]\n",
      " [ 5 51  4 46 33 33 45 31]\n",
      " [71 74 74  1 22  5 71 74]\n",
      " [ 1  5 53 63 29 71 25 51]\n",
      " [27  4 22 65 71 12  5 71]\n",
      " [ 5  4 10  5 71  5 36 39]\n",
      " [ 5 53 71 39 22 12  5 71]\n",
      " [25  5 71 25 22  5 40 59]\n",
      " [ 5  4 10  5 63 64  1  5]\n",
      " [63  5 53 63 71 39 29 41]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print(\"cuda is available\")\n",
    "else:\n",
    "    print(\"training on cpu \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y).long()\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    targets = targets.long()\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(76, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=76, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2391... Val Loss: 3.2153\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1613... Val Loss: 3.1432\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1309... Val Loss: 3.1325\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1202... Val Loss: 3.1296\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1134... Val Loss: 3.1270\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1002... Val Loss: 3.1236\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1092... Val Loss: 3.1163\n",
      "Epoch: 2/20... Step: 80... Loss: 3.0984... Val Loss: 3.1012\n",
      "Epoch: 2/20... Step: 90... Loss: 3.0673... Val Loss: 3.0763\n",
      "Epoch: 2/20... Step: 100... Loss: 2.9926... Val Loss: 3.0025\n",
      "Epoch: 2/20... Step: 110... Loss: 2.8847... Val Loss: 2.9044\n",
      "Epoch: 2/20... Step: 120... Loss: 2.7581... Val Loss: 2.7557\n",
      "Epoch: 2/20... Step: 130... Loss: 2.6702... Val Loss: 2.6593\n",
      "Epoch: 2/20... Step: 140... Loss: 2.5850... Val Loss: 2.5635\n",
      "Epoch: 3/20... Step: 150... Loss: 2.5349... Val Loss: 2.5061\n",
      "Epoch: 3/20... Step: 160... Loss: 2.4915... Val Loss: 2.4507\n",
      "Epoch: 3/20... Step: 170... Loss: 2.4271... Val Loss: 2.4217\n",
      "Epoch: 3/20... Step: 180... Loss: 2.4011... Val Loss: 2.3777\n",
      "Epoch: 3/20... Step: 190... Loss: 2.3570... Val Loss: 2.3395\n",
      "Epoch: 3/20... Step: 200... Loss: 2.3470... Val Loss: 2.3128\n",
      "Epoch: 3/20... Step: 210... Loss: 2.3242... Val Loss: 2.2685\n",
      "Epoch: 4/20... Step: 220... Loss: 2.2711... Val Loss: 2.2366\n",
      "Epoch: 4/20... Step: 230... Loss: 2.2606... Val Loss: 2.2004\n",
      "Epoch: 4/20... Step: 240... Loss: 2.2038... Val Loss: 2.1548\n",
      "Epoch: 4/20... Step: 250... Loss: 2.2025... Val Loss: 2.1355\n",
      "Epoch: 4/20... Step: 260... Loss: 2.1594... Val Loss: 2.1030\n",
      "Epoch: 4/20... Step: 270... Loss: 2.1481... Val Loss: 2.0812\n",
      "Epoch: 4/20... Step: 280... Loss: 2.1437... Val Loss: 2.0428\n",
      "Epoch: 5/20... Step: 290... Loss: 2.0873... Val Loss: 2.0187\n",
      "Epoch: 5/20... Step: 300... Loss: 2.1038... Val Loss: 1.9904\n",
      "Epoch: 5/20... Step: 310... Loss: 2.0485... Val Loss: 1.9624\n",
      "Epoch: 5/20... Step: 320... Loss: 2.0447... Val Loss: 1.9389\n",
      "Epoch: 5/20... Step: 330... Loss: 2.0135... Val Loss: 1.9159\n",
      "Epoch: 5/20... Step: 340... Loss: 2.0251... Val Loss: 1.8997\n",
      "Epoch: 5/20... Step: 350... Loss: 2.0151... Val Loss: 1.8778\n",
      "Epoch: 6/20... Step: 360... Loss: 1.9520... Val Loss: 1.8616\n",
      "Epoch: 6/20... Step: 370... Loss: 1.9817... Val Loss: 1.8516\n",
      "Epoch: 6/20... Step: 380... Loss: 1.9178... Val Loss: 1.8233\n",
      "Epoch: 6/20... Step: 390... Loss: 1.9384... Val Loss: 1.8071\n",
      "Epoch: 6/20... Step: 400... Loss: 1.9094... Val Loss: 1.7911\n",
      "Epoch: 6/20... Step: 410... Loss: 1.9168... Val Loss: 1.7750\n",
      "Epoch: 6/20... Step: 420... Loss: 1.9263... Val Loss: 1.7617\n",
      "Epoch: 7/20... Step: 430... Loss: 1.8603... Val Loss: 1.7483\n",
      "Epoch: 7/20... Step: 440... Loss: 1.8761... Val Loss: 1.7331\n",
      "Epoch: 7/20... Step: 450... Loss: 1.8215... Val Loss: 1.7184\n",
      "Epoch: 7/20... Step: 460... Loss: 1.8437... Val Loss: 1.7070\n",
      "Epoch: 7/20... Step: 470... Loss: 1.8122... Val Loss: 1.6954\n",
      "Epoch: 7/20... Step: 480... Loss: 1.8299... Val Loss: 1.6810\n",
      "Epoch: 7/20... Step: 490... Loss: 1.8351... Val Loss: 1.6719\n",
      "Epoch: 8/20... Step: 500... Loss: 1.7695... Val Loss: 1.6605\n",
      "Epoch: 8/20... Step: 510... Loss: 1.7938... Val Loss: 1.6491\n",
      "Epoch: 8/20... Step: 520... Loss: 1.7409... Val Loss: 1.6359\n",
      "Epoch: 8/20... Step: 530... Loss: 1.7602... Val Loss: 1.6264\n",
      "Epoch: 8/20... Step: 540... Loss: 1.7400... Val Loss: 1.6187\n",
      "Epoch: 8/20... Step: 550... Loss: 1.7580... Val Loss: 1.6048\n",
      "Epoch: 8/20... Step: 560... Loss: 1.7669... Val Loss: 1.6007\n",
      "Epoch: 9/20... Step: 570... Loss: 1.7031... Val Loss: 1.5896\n",
      "Epoch: 9/20... Step: 580... Loss: 1.7349... Val Loss: 1.5850\n",
      "Epoch: 9/20... Step: 590... Loss: 1.6786... Val Loss: 1.5748\n",
      "Epoch: 9/20... Step: 600... Loss: 1.7034... Val Loss: 1.5650\n",
      "Epoch: 9/20... Step: 610... Loss: 1.6817... Val Loss: 1.5600\n",
      "Epoch: 9/20... Step: 620... Loss: 1.7060... Val Loss: 1.5495\n",
      "Epoch: 9/20... Step: 630... Loss: 1.7233... Val Loss: 1.5452\n",
      "Epoch: 10/20... Step: 640... Loss: 1.6522... Val Loss: 1.5351\n",
      "Epoch: 10/20... Step: 650... Loss: 1.6737... Val Loss: 1.5337\n",
      "Epoch: 10/20... Step: 660... Loss: 1.6228... Val Loss: 1.5225\n",
      "Epoch: 10/20... Step: 670... Loss: 1.6450... Val Loss: 1.5128\n",
      "Epoch: 10/20... Step: 680... Loss: 1.6319... Val Loss: 1.5092\n",
      "Epoch: 10/20... Step: 690... Loss: 1.6591... Val Loss: 1.4997\n",
      "Epoch: 10/20... Step: 700... Loss: 1.6669... Val Loss: 1.4962\n",
      "Epoch: 11/20... Step: 710... Loss: 1.6034... Val Loss: 1.4899\n",
      "Epoch: 11/20... Step: 720... Loss: 1.6363... Val Loss: 1.4846\n",
      "Epoch: 11/20... Step: 730... Loss: 1.5644... Val Loss: 1.4773\n",
      "Epoch: 11/20... Step: 740... Loss: 1.6038... Val Loss: 1.4713\n",
      "Epoch: 11/20... Step: 750... Loss: 1.5918... Val Loss: 1.4670\n",
      "Epoch: 11/20... Step: 760... Loss: 1.6131... Val Loss: 1.4611\n",
      "Epoch: 11/20... Step: 770... Loss: 1.6294... Val Loss: 1.4572\n",
      "Epoch: 12/20... Step: 780... Loss: 1.5756... Val Loss: 1.4535\n",
      "Epoch: 12/20... Step: 790... Loss: 1.6016... Val Loss: 1.4486\n",
      "Epoch: 12/20... Step: 800... Loss: 1.5278... Val Loss: 1.4421\n",
      "Epoch: 12/20... Step: 810... Loss: 1.5565... Val Loss: 1.4367\n",
      "Epoch: 12/20... Step: 820... Loss: 1.5508... Val Loss: 1.4317\n",
      "Epoch: 12/20... Step: 830... Loss: 1.5762... Val Loss: 1.4285\n",
      "Epoch: 12/20... Step: 840... Loss: 1.5999... Val Loss: 1.4247\n",
      "Epoch: 13/20... Step: 850... Loss: 1.5293... Val Loss: 1.4201\n",
      "Epoch: 13/20... Step: 860... Loss: 1.5629... Val Loss: 1.4146\n",
      "Epoch: 13/20... Step: 870... Loss: 1.4896... Val Loss: 1.4107\n",
      "Epoch: 13/20... Step: 880... Loss: 1.5287... Val Loss: 1.4063\n",
      "Epoch: 13/20... Step: 890... Loss: 1.5295... Val Loss: 1.4013\n",
      "Epoch: 13/20... Step: 900... Loss: 1.5477... Val Loss: 1.3998\n",
      "Epoch: 13/20... Step: 910... Loss: 1.5647... Val Loss: 1.3966\n",
      "Epoch: 14/20... Step: 920... Loss: 1.5074... Val Loss: 1.3900\n",
      "Epoch: 14/20... Step: 930... Loss: 1.5288... Val Loss: 1.3868\n",
      "Epoch: 14/20... Step: 940... Loss: 1.4525... Val Loss: 1.3844\n",
      "Epoch: 14/20... Step: 950... Loss: 1.5004... Val Loss: 1.3804\n",
      "Epoch: 14/20... Step: 960... Loss: 1.4927... Val Loss: 1.3757\n",
      "Epoch: 14/20... Step: 970... Loss: 1.5194... Val Loss: 1.3749\n",
      "Epoch: 14/20... Step: 980... Loss: 1.5326... Val Loss: 1.3696\n",
      "Epoch: 15/20... Step: 990... Loss: 1.4859... Val Loss: 1.3683\n",
      "Epoch: 15/20... Step: 1000... Loss: 1.4904... Val Loss: 1.3651\n",
      "Epoch: 15/20... Step: 1010... Loss: 1.4175... Val Loss: 1.3631\n",
      "Epoch: 15/20... Step: 1020... Loss: 1.4751... Val Loss: 1.3588\n",
      "Epoch: 15/20... Step: 1030... Loss: 1.4639... Val Loss: 1.3529\n",
      "Epoch: 15/20... Step: 1040... Loss: 1.4886... Val Loss: 1.3531\n",
      "Epoch: 15/20... Step: 1050... Loss: 1.5077... Val Loss: 1.3487\n",
      "Epoch: 16/20... Step: 1060... Loss: 1.4557... Val Loss: 1.3501\n",
      "Epoch: 16/20... Step: 1070... Loss: 1.4702... Val Loss: 1.3437\n",
      "Epoch: 16/20... Step: 1080... Loss: 1.3977... Val Loss: 1.3440\n",
      "Epoch: 16/20... Step: 1090... Loss: 1.4562... Val Loss: 1.3439\n",
      "Epoch: 16/20... Step: 1100... Loss: 1.4470... Val Loss: 1.3356\n",
      "Epoch: 16/20... Step: 1110... Loss: 1.4729... Val Loss: 1.3361\n",
      "Epoch: 16/20... Step: 1120... Loss: 1.4981... Val Loss: 1.3321\n",
      "Epoch: 17/20... Step: 1130... Loss: 1.4360... Val Loss: 1.3306\n",
      "Epoch: 17/20... Step: 1140... Loss: 1.4477... Val Loss: 1.3263\n",
      "Epoch: 17/20... Step: 1150... Loss: 1.3782... Val Loss: 1.3256\n",
      "Epoch: 17/20... Step: 1160... Loss: 1.4289... Val Loss: 1.3245\n",
      "Epoch: 17/20... Step: 1170... Loss: 1.4233... Val Loss: 1.3201\n",
      "Epoch: 17/20... Step: 1180... Loss: 1.4503... Val Loss: 1.3190\n",
      "Epoch: 17/20... Step: 1190... Loss: 1.4625... Val Loss: 1.3155\n",
      "Epoch: 18/20... Step: 1200... Loss: 1.4206... Val Loss: 1.3131\n",
      "Epoch: 18/20... Step: 1210... Loss: 1.4311... Val Loss: 1.3109\n",
      "Epoch: 18/20... Step: 1220... Loss: 1.3610... Val Loss: 1.3108\n",
      "Epoch: 18/20... Step: 1230... Loss: 1.4131... Val Loss: 1.3084\n",
      "Epoch: 18/20... Step: 1240... Loss: 1.4014... Val Loss: 1.3068\n",
      "Epoch: 18/20... Step: 1250... Loss: 1.4323... Val Loss: 1.3078\n",
      "Epoch: 18/20... Step: 1260... Loss: 1.4516... Val Loss: 1.3040\n",
      "Epoch: 19/20... Step: 1270... Loss: 1.4040... Val Loss: 1.3014\n",
      "Epoch: 19/20... Step: 1280... Loss: 1.4133... Val Loss: 1.3010\n",
      "Epoch: 19/20... Step: 1290... Loss: 1.3445... Val Loss: 1.2975\n",
      "Epoch: 19/20... Step: 1300... Loss: 1.3916... Val Loss: 1.2970\n",
      "Epoch: 19/20... Step: 1310... Loss: 1.3873... Val Loss: 1.2924\n",
      "Epoch: 19/20... Step: 1320... Loss: 1.4142... Val Loss: 1.2896\n",
      "Epoch: 19/20... Step: 1330... Loss: 1.4240... Val Loss: 1.2876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 1340... Loss: 1.3856... Val Loss: 1.2881\n",
      "Epoch: 20/20... Step: 1350... Loss: 1.3968... Val Loss: 1.2845\n",
      "Epoch: 20/20... Step: 1360... Loss: 1.3200... Val Loss: 1.2856\n",
      "Epoch: 20/20... Step: 1370... Loss: 1.3753... Val Loss: 1.2854\n",
      "Epoch: 20/20... Step: 1380... Loss: 1.3709... Val Loss: 1.2797\n",
      "Epoch: 20/20... Step: 1390... Loss: 1.3962... Val Loss: 1.2790\n",
      "Epoch: 20/20... Step: 1400... Loss: 1.4201... Val Loss: 1.2805\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_x_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, she was standing all the meaton and treished that sort\n",
      "in this some, who had\n",
      "certainly he seined to the marriage as the sones, had been\n",
      "straight on, and all that he went all at the position, and short there was no too her hortes which\n",
      "though he's a from him with a children and the sectnate time of the saming of his. \"It were if a meadow, I did not sungrittent and the strong what was\n",
      "no since that's not a shand is time. I didn't see, to do. And so that I could never me so true.\"\n",
      "\n",
      "\"What's why to do?\" answered Anna had been to say to him, and there was\n",
      "an that in her servage to still\n",
      "all this was the plooded without thinking, that he succeeded a sickly\n",
      "started tinnersely.\n",
      "\n",
      "\"I don't know it and though it must himself to take you was not in the prince as too things that I'm\n",
      "tried on that, I'll tell you that to me her and I sold to me her, and shall never more that it was a long always a sould will, think of his wife,\n",
      "and he has say,\" he asked that he had not havid state, but he had not tre\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open(model_name, 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said that whom he heard the constion went to be a simple still as in some the wors and a studing often and sont of suck her eyes that this with a sturge into her hands. She\n",
      "was\n",
      "coushed\n",
      "to herself how she was a letter, he was\n",
      "the come with his face on the peasant\n",
      "servent with her.\n",
      "\n",
      "\"Well, to that's it?\"\n",
      "\n",
      "\"No,, and wantted to her.\"\n",
      "\n",
      "\"What went to be a sincal of mose are,\" he said, and who had been the back to to make\n",
      "him\n",
      "to say he without\n",
      "spite, and his\n",
      "choored has a partly and takes with supper, and had been\n",
      "delanitaliar was to be so that\n",
      "though he\n",
      "was at it.\n",
      "\n",
      "\"And I'v not could so like the mose as a crande stain outs.\"\n",
      "\n",
      "\"No, I, it said that...\" he said, and tried to\n",
      "bo at an englasing of any horeself were tightly as a land of a bell to his brother the same was starking that has brought all his wife, souched her sorts, and\n",
      "there saw all her attonciously\n",
      "shooking over hards\n",
      "which all his strange to the proncess hid\n",
      "before the recoldection with a brown,\n",
      "bus all\n",
      "the same she was the confidence was the party and happened. \"It's the married all the same tog a sorper so the same them of the passer were broken. I see it always be seling at anything.\"\n",
      "\n",
      "\"You knew that I don't deciding it,\" and she consciously went time to\n",
      "have been drinking to her. \"You see the sense of it,\" he asked, with her\n",
      "sout, the cate of the race he couldnno have been his\n",
      "seemed with the world had to go to truit,\n",
      "but it was so stood in a secingrate of his frame of him to be short shoot and hinging at an offer, and he was any heart, without concisiante it\n",
      "and wife shill out\n",
      "of the most\n",
      "only shalf was a little while in the part of the room,\n",
      "and he was that he could be thinging any anywhen at the\n",
      "timid of\n",
      "the children to such the carriage to the peasant of her heart and his hind,\n",
      "but the more without the mowers was not to see that that stopped her that to say and would should be to soon an\n",
      "instant when their way\n",
      "to the say well was in suppose of her forestes in with his head.\n",
      "\n",
      "\"I so as always all a starded, ti\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
